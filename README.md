# VLM-RL: Vision-Language Model Reinforcement Learning for Autonomous Driving

This project is an implementation of the research paper [**"VLM-RL: A Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving"**](https://github.com/zihaosheng/VLM-RL). The VLM-RL framework integrates Vision-Language Models (VLMs) with Reinforcement Learning (RL) to improve the decision-making of autonomous driving agents. This hybrid framework uses Contrastive Language Goals (CLG) to guide the agent with high-level semantic goals like collision avoidance and lane-keeping, which are derived from natural language descriptions. By replacing traditional RL rewards with semantic guidance, VLM-RL aims to improve the safety and reliability of autonomous driving systems.
